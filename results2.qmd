# Refined Results

Reviewed the current literature on selection of candidate learners to understand why TMLE was not achieving best coverage (particularly double cross-fitted version). 

::: column-margin
[@naimi2017challenges; @balzer2021demystifying; @Phillips2021; @zivich2021machine]
::: 

::: callout-tip
# Donsker class and Smooth learner
- **Donsker class**: uniform convergence of empirical processes, generalization performance of learning algorithms, reduction of the risk of overfitting
- **Smooth learner**: algorithm producing differentiable and continuous functions. Non-smooth are often sensitive to small changes in the training data and can overfit easily.

1. Logistic regression: smooth learner, belongs to the Donsker class
2. MARS: piecewise smooth learner, classification of MARS into the Donsker class is not straightforward 
3. LASSO: produces a continuous function, can be considered piecewise smooth or quasi-smooth; can belong to the Donsker class
4. XGBoost: <mark>non-smooth learner</mark>, classification of XGBoost into the Donsker class is not straightforward

Super Learner guidelines suggested using diverse learners, including both smooth and non-smooth learners. The main idea behind this recommendation was to leverage the strengths of different learners and allow them to compensate for each other's weaknesses. By combining diverse base learners, Super Learner aims to improve the overall generalization performance of the ensemble. However, researchers and practitioners have gained a deeper understanding of the implications of including non-smooth learners in the ensemble. The potential downsides of non-smooth learners, such as high variance and overfitting, are now better understood. As a result, more emphasis may be placed on carefully selecting and tuning non-smooth learners to minimize their potential negative impact on the ensemble's performance.
:::


## Bias

```{r fig.cap="", echo=FALSE, out.width="80%", out.height="auto"}
knitr::include_graphics("images/waS/bias.png")
```

::: column-margin
-   `SL` represents those where super learner was used with the following 3 candidate learners
    1.  Logistic regression
    2.  MARS (Multivariate Adaptive Regression Splines)
    3.  LASSO
-   `TMLE` represents those where TMLE was used
-   `DC` represents double cross-fit.

<mark>XGBoost omitted</mark>
:::

::: column-margin
- `DC` version of `TMLE` (kitchen sink) associated with least bias!
- Non-super learner methods remains the same (results did not change).
- Same super learner used for `SL` and `TMLE` methods.
:::

## MSE

```{r fig.cap="", echo=FALSE, out.width="80%", out.height="auto"}
knitr::include_graphics("images/waS/mse.png")
```

::: column-margin
- `DC` version of `TMLE` (kitchen sink) associated with least MSE!
::: 

## Relative Error

```{r fig.cap="", echo=FALSE, out.width="80%", out.height="auto"}
knitr::include_graphics("images/waS/relerror.png")
```

::: column-margin
- `DC` version of `TMLE` (kitchen sink) associated with least relative error in model SE estimation!
::: 

## Coverage

```{r fig.cap="", echo=FALSE, out.width="80%", out.height="auto"}
knitr::include_graphics("images/waS/cover.png")
```

::: column-margin
- `DC` version of `TMLE` (kitchen sink) associated with coverage closest to nominal!
::: 

## Bias eliminated coverage

```{r fig.cap="", echo=FALSE, out.width="80%", out.height="auto"}
knitr::include_graphics("images/waS/becover.png")
```

::: column-margin
- `DC` version of `TMLE` (kitchen sink) associated with bias eliminated coverage closest to nominal!
::: 

## More Details

Review more detailed simulation results conducted:

```{=html}
<style>
  table {
    border-collapse: collapse;
    width: 100%;
  }
  
  th, td {
    padding: 8px;
    text-align: left;
    border-bottom: 1px solid #ddd;
  }

  tr:nth-child(odd) {
    background-color: #f2f2f2;
  }

  tr:nth-child(even) td:nth-child(1){
    background-color: #f2f2f2;
  }
  
  th {
    border-top: 1px solid #ddd;
    border-bottom: 1px solid #ddd;
  }
</style>
```

| Analysis strategy | Outcome Model Specification |
| --- | --- |
| Firth regression | The same |
| Stabilized weights | The same |
| Various outcome model specification | |
| | No covariate adjustment |
| | Just investigator-specified covariate adjustment |
| | Adjustment of only those investigator-specified covariates that are imbalanced (SMD > 0.1) |
| | Adjustment of only those covariates that are imbalanced (SMD > 0.1; investigator-specified or recurrence) |
| | Adjustment of all covariates (investigator-specified or recurrence) |


::: column-margin
[@firth1993bias; @austin2015moving]

All of the aove figures are based on <mark>adjustment of all covariates</mark> (investigator-specified or recurrence) in the outcome model.
:::

```{=html}
<!-- 
<iframe src="https://ehsanx.shinyapps.io/"
        style="border:none;width:100%;height:500px;"></iframe>
-->
```


::: callout-tip
# ShinyApp
Look at the [ShinyApp](https://ehsanx.shinyapps.io/hdPSsim/) for additional details!
::: 